{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrederickUdis/Arboles-ejercicio/blob/master/2_%2BFundamentos%2Bde%2BSpark_SQL_DataFrames.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRThDiA6Anr-"
      },
      "source": [
        "# Fundamentos de Apache Spark: SQL/DataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kVyn2yVAnsB"
      },
      "source": [
        "**Spark** SQL opera con **DataFrames**. Un **DataFrame** es una visualización relacional de la información. Ofrece funciones con características parecidas a SQL. También, posibilita redactar interrogantes al estilo SQL para nuestra exploración de datos.\n",
        "\n",
        "Los **DataFrames** se asemejan a las tablas relacionales o a los DataFrames en Python/R, aunque con numerosas mejoras que se llevan a cabo de forma \"transparente\" para el usuario. Existen múltiples métodos para generar DataFrames desde conjuntos, tablas HIVE, tablas relacionales y RDD."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalacion de la libreria findspark"
      ],
      "metadata": {
        "id": "9kONo_NlIr9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YSiQJBQIGti",
        "outputId": "e6766068-7819-4da5-f185-94225c93dc59"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "fw4yoCmhAnsC",
        "outputId": "c67f6ee7-9cdc-4c8f-e640-bc0d42fbfe01"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7c56fb6583b9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'findspark'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "import pandas as pd\n",
        "import pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZDaVrv6AnsD"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSBRZQ_4AnsD"
      },
      "source": [
        "### Crear la sesión de Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uoQdWqOAnsD"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXrkb26MAnsE"
      },
      "source": [
        "### Crear el DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAqnE8FzAnsE"
      },
      "outputs": [],
      "source": [
        "emp = [(1, \"AAA\", \"dept1\", 1000),\n",
        "    (2, \"BBB\", \"dept1\", 1100),\n",
        "    (3, \"CCC\", \"dept1\", 3000),\n",
        "    (4, \"DDD\", \"dept1\", 1500),\n",
        "    (5, \"EEE\", \"dept2\", 8000),\n",
        "    (6, \"FFF\", \"dept2\", 7200),\n",
        "    (7, \"GGG\", \"dept3\", 7100),\n",
        "    (8, \"HHH\", \"dept3\", 3700),\n",
        "    (9, \"III\", \"dept3\", 4500),\n",
        "    (10, \"JJJ\", \"dept5\", 3400)]\n",
        "\n",
        "dept = [(\"dept1\", \"Department - 1\"),\n",
        "        (\"dept2\", \"Department - 2\"),\n",
        "        (\"dept3\", \"Department - 3\"),\n",
        "        (\"dept4\", \"Department - 4\")\n",
        "\n",
        "       ]\n",
        "\n",
        "df = spark.createDataFrame(emp, [\"id\", \"name\", \"dept\", \"salary\"])\n",
        "\n",
        "deptdf = spark.createDataFrame(dept, [\"id\", \"name\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bn3oQIxfAnsE",
        "outputId": "e5e04176-f494-4e3b-f70f-7db135ec79fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  1| AAA|dept1|  1000|\n",
            "|  2| BBB|dept1|  1100|\n",
            "|  3| CCC|dept1|  3000|\n",
            "|  4| DDD|dept1|  1500|\n",
            "|  5| EEE|dept2|  8000|\n",
            "|  6| FFF|dept2|  7200|\n",
            "|  7| GGG|dept3|  7100|\n",
            "|  8| HHH|dept3|  3700|\n",
            "|  9| III|dept3|  4500|\n",
            "| 10| JJJ|dept5|  3400|\n",
            "+---+----+-----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uxt5x41AnsF"
      },
      "outputs": [],
      "source": [
        "#Crear un df a partir de una tabla de Hive\n",
        "df = spark.table(“tbl_name”)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjyumsakAnsF"
      },
      "source": [
        "# Operaciones básicas en DataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSMLDlDEAnsG"
      },
      "source": [
        "### count\n",
        "* Cuenta el número de filas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Yx_ihJzAnsG",
        "outputId": "2dcdccb1-6f4f-448c-f2a6-b3e2e3b6c1e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jo2RBRUAnsG"
      },
      "source": [
        "### columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gl48-feuAnsG",
        "outputId": "4ee5efdf-1a9d-40cb-9927-55ed012aaf23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['id', 'name', 'dept', 'salary']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YFOweBLAnsH"
      },
      "source": [
        "### dtypes\n",
        "** Accede al DataType de columnas dentro del DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BovczhlSAnsH",
        "outputId": "3071987b-db50-42ab-f537-aaa7e7b6877d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('id', 'bigint'),\n",
              " ('name', 'string'),\n",
              " ('dept', 'string'),\n",
              " ('salary', 'bigint')]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiKr5lw3AnsH"
      },
      "source": [
        "### schema\n",
        "** Comprueba cómo Spark almacena el esquema del DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ay1nQB3vAnsH",
        "outputId": "c522502e-8c62-4f53-c286-0d577b728254"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "StructType(List(StructField(id,LongType,true),StructField(name,StringType,true),StructField(dept,StringType,true),StructField(salary,LongType,true)))"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.schema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB41AAOBAnsH"
      },
      "source": [
        "### printSchema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yGlySSIAnsI",
        "outputId": "578e6fd2-6268-45a1-ff63-5015effcfd75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- dept: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F9USFqjAnsI"
      },
      "source": [
        "### select\n",
        "* Seleccione columnas del DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIa1LmaGAnsI",
        "outputId": "175c2a3f-d486-4493-9199-aa18bac2a50c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----+\n",
            "| id|name|\n",
            "+---+----+\n",
            "|  1| AAA|\n",
            "|  2| BBB|\n",
            "|  3| CCC|\n",
            "|  4| DDD|\n",
            "|  5| EEE|\n",
            "|  6| FFF|\n",
            "|  7| GGG|\n",
            "|  8| HHH|\n",
            "|  9| III|\n",
            "| 10| JJJ|\n",
            "+---+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(\"id\", \"name\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIKfYoY0AnsI"
      },
      "source": [
        "### filter\n",
        "\n",
        "* Filtrar las filas según alguna condición.\n",
        "* Intentemos encontrar las filas con id = 1.\n",
        "* Hay diferentes formas de especificar la condición."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yL6kLkhAAnsI",
        "outputId": "e06830fe-c753-4be6-e674-f9ffc07867da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  1| AAA|dept1|  1000|\n",
            "+---+----+-----+------+\n",
            "\n",
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  1| AAA|dept1|  1000|\n",
            "+---+----+-----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.filter(df[\"id\"] == 1).show()\n",
        "df.filter(df.id == 1).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omGzW4zVAnsJ",
        "outputId": "179a6fe4-4383-4402-9d05-abb17ca66b83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  1| AAA|dept1|  1000|\n",
            "+---+----+-----+------+\n",
            "\n",
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  1| AAA|dept1|  1000|\n",
            "+---+----+-----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.filter(col(\"id\") == 1).show()\n",
        "df.filter(\"id = 1\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ehJTlPzAnsJ"
      },
      "source": [
        "### drop\n",
        "* Elimina una columna en particular"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQnK9yA8AnsJ",
        "outputId": "11ffcacc-10f6-4a91-e111-9aadf0fcc28c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+-----+------+\n",
            "|name| dept|salary|\n",
            "+----+-----+------+\n",
            "| AAA|dept1|  1000|\n",
            "| BBB|dept1|  1100|\n",
            "+----+-----+------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "newdf = df.drop(\"id\")\n",
        "newdf.show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf9h7Nn-AnsJ"
      },
      "source": [
        "### Aggregations\n",
        "* Podemos usar la función groupBy para agrupar los datos y luego usar la función \"agg\" para realizar la agregación de datos agrupados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83twCX-LAnsK",
        "outputId": "0c240de0-efa2-4b0f-a6fc-5521a139516e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+-----+-----+----+----+------+\n",
            "| dept|count|  sum| max| min|   avg|\n",
            "+-----+-----+-----+----+----+------+\n",
            "|dept5|    1| 3400|3400|3400|3400.0|\n",
            "|dept3|    3|15300|7100|3700|5100.0|\n",
            "|dept1|    4| 6600|3000|1000|1650.0|\n",
            "|dept2|    2|15200|8000|7200|7600.0|\n",
            "+-----+-----+-----+----+----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "(df.groupBy(\"dept\")\n",
        "    .agg(\n",
        "        count(\"salary\").alias(\"count\"),\n",
        "        sum(\"salary\").alias(\"sum\"),\n",
        "        max(\"salary\").alias(\"max\"),\n",
        "        min(\"salary\").alias(\"min\"),\n",
        "        avg(\"salary\").alias(\"avg\")\n",
        "        ).show()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9EET8SdAnsK"
      },
      "source": [
        "### Sorting\n",
        "\n",
        "* Ordena los datos según el \"salario\". De forma predeterminada, la clasificación se realizará en orden ascendente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZj5rvojAnsK",
        "outputId": "95721d9c-880f-4cfe-ff01-9bf1f75fdb72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  1| AAA|dept1|  1000|\n",
            "|  2| BBB|dept1|  1100|\n",
            "|  4| DDD|dept1|  1500|\n",
            "|  3| CCC|dept1|  3000|\n",
            "| 10| JJJ|dept5|  3400|\n",
            "+---+----+-----+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.sort(\"salary\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqDzDsMRAnsK",
        "outputId": "57281209-6777-42e7-9e4d-61f13fbb3066"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  5| EEE|dept2|  8000|\n",
            "|  6| FFF|dept2|  7200|\n",
            "|  7| GGG|dept3|  7100|\n",
            "|  9| III|dept3|  4500|\n",
            "|  8| HHH|dept3|  3700|\n",
            "+---+----+-----+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sort the data in descending order.\n",
        "df.sort(desc(\"salary\")).show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Oyq9J4jAnsK"
      },
      "source": [
        "### Columnas derivadas\n",
        "* Podemos usar la función \"withColumn\" para derivar la columna en función de las columnas existentes ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qf6hcxt2AnsL",
        "outputId": "7dd23fb0-0f12-4062-8b40-c53d64d82260"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----+-----+------+-----+\n",
            "| id|name| dept|salary|bonus|\n",
            "+---+----+-----+------+-----+\n",
            "|  1| AAA|dept1|  1000|100.0|\n",
            "|  2| BBB|dept1|  1100|110.0|\n",
            "|  3| CCC|dept1|  3000|300.0|\n",
            "|  4| DDD|dept1|  1500|150.0|\n",
            "|  5| EEE|dept2|  8000|800.0|\n",
            "|  6| FFF|dept2|  7200|720.0|\n",
            "|  7| GGG|dept3|  7100|710.0|\n",
            "|  8| HHH|dept3|  3700|370.0|\n",
            "|  9| III|dept3|  4500|450.0|\n",
            "| 10| JJJ|dept5|  3400|340.0|\n",
            "+---+----+-----+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.withColumn(\"bonus\", col(\"salary\") * .1).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9jVwGfyAnsL"
      },
      "source": [
        "### Joins\n",
        "\n",
        "* Podemos realizar varios tipos de combinaciones en múltiples DataFrames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI8mkZ69AnsL"
      },
      "source": [
        "![Sin%20t%C3%ADtulo.png](attachment:Sin%20t%C3%ADtulo.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpgKzlhnAnsL",
        "outputId": "d667d8a2-eb3f-48ca-b8e0-4bbc4e241832"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----+-----+------+-----+--------------+\n",
            "| id|name| dept|salary|   id|          name|\n",
            "+---+----+-----+------+-----+--------------+\n",
            "|  7| GGG|dept3|  7100|dept3|Department - 3|\n",
            "|  8| HHH|dept3|  3700|dept3|Department - 3|\n",
            "|  9| III|dept3|  4500|dept3|Department - 3|\n",
            "|  1| AAA|dept1|  1000|dept1|Department - 1|\n",
            "|  2| BBB|dept1|  1100|dept1|Department - 1|\n",
            "|  3| CCC|dept1|  3000|dept1|Department - 1|\n",
            "|  4| DDD|dept1|  1500|dept1|Department - 1|\n",
            "|  5| EEE|dept2|  8000|dept2|Department - 2|\n",
            "|  6| FFF|dept2|  7200|dept2|Department - 2|\n",
            "+---+----+-----+------+-----+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Inner JOIN.\n",
        "df.join(deptdf, df[\"dept\"] == deptdf[\"id\"]).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02zrw0DsAnsM"
      },
      "source": [
        "### Left Outer Join"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7U_6f4RAnsX",
        "outputId": "24354fc9-4cc4-4b9b-cc9f-50c49bcd4c94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----+-----+------+-----+--------------+\n",
            "| id|name| dept|salary|   id|          name|\n",
            "+---+----+-----+------+-----+--------------+\n",
            "| 10| JJJ|dept5|  3400| null|          null|\n",
            "|  7| GGG|dept3|  7100|dept3|Department - 3|\n",
            "|  8| HHH|dept3|  3700|dept3|Department - 3|\n",
            "|  9| III|dept3|  4500|dept3|Department - 3|\n",
            "|  1| AAA|dept1|  1000|dept1|Department - 1|\n",
            "|  2| BBB|dept1|  1100|dept1|Department - 1|\n",
            "|  3| CCC|dept1|  3000|dept1|Department - 1|\n",
            "|  4| DDD|dept1|  1500|dept1|Department - 1|\n",
            "|  5| EEE|dept2|  8000|dept2|Department - 2|\n",
            "|  6| FFF|dept2|  7200|dept2|Department - 2|\n",
            "+---+----+-----+------+-----+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.join(deptdf, df[\"dept\"] == deptdf[\"id\"], \"left_outer\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C06NozkdAnsX"
      },
      "source": [
        "### Right Outer Join"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zX2o_dIMAnsX",
        "outputId": "84626e11-efb5-49e3-808e-48937534f0a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----+-----+------+-----+--------------+\n",
            "|  id|name| dept|salary|   id|          name|\n",
            "+----+----+-----+------+-----+--------------+\n",
            "|   7| GGG|dept3|  7100|dept3|Department - 3|\n",
            "|   8| HHH|dept3|  3700|dept3|Department - 3|\n",
            "|   9| III|dept3|  4500|dept3|Department - 3|\n",
            "|   1| AAA|dept1|  1000|dept1|Department - 1|\n",
            "|   2| BBB|dept1|  1100|dept1|Department - 1|\n",
            "|   3| CCC|dept1|  3000|dept1|Department - 1|\n",
            "|   4| DDD|dept1|  1500|dept1|Department - 1|\n",
            "|null|null| null|  null|dept4|Department - 4|\n",
            "|   5| EEE|dept2|  8000|dept2|Department - 2|\n",
            "|   6| FFF|dept2|  7200|dept2|Department - 2|\n",
            "+----+----+-----+------+-----+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.join(deptdf, df[\"dept\"] == deptdf[\"id\"], \"right_outer\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8eBJiwOAnsX"
      },
      "source": [
        "### Full Outer Join"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unDltcxgAnsX",
        "outputId": "c126c945-9b02-4594-91bd-1201b4a419c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----+-----+------+-----+--------------+\n",
            "|  id|name| dept|salary|   id|          name|\n",
            "+----+----+-----+------+-----+--------------+\n",
            "|  10| JJJ|dept5|  3400| null|          null|\n",
            "|   7| GGG|dept3|  7100|dept3|Department - 3|\n",
            "|   8| HHH|dept3|  3700|dept3|Department - 3|\n",
            "|   9| III|dept3|  4500|dept3|Department - 3|\n",
            "|   1| AAA|dept1|  1000|dept1|Department - 1|\n",
            "|   2| BBB|dept1|  1100|dept1|Department - 1|\n",
            "|   3| CCC|dept1|  3000|dept1|Department - 1|\n",
            "|   4| DDD|dept1|  1500|dept1|Department - 1|\n",
            "|null|null| null|  null|dept4|Department - 4|\n",
            "|   5| EEE|dept2|  8000|dept2|Department - 2|\n",
            "|   6| FFF|dept2|  7200|dept2|Department - 2|\n",
            "+----+----+-----+------+-----+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.join(deptdf, df[\"dept\"] == deptdf[\"id\"], \"outer\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAh0_vBaAnsY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlgDksGMAnsY"
      },
      "source": [
        "### Consultas SQL\n",
        "* Ejecución de consultas tipo SQL.\n",
        "* También podemos realizar análisis de datos escribiendo consultas similares a SQL. Para realizar consultas similares a SQL, necesitamos registrar el DataFrame como una Vista temporal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R636dWTJAnsY",
        "outputId": "bd6d26b1-1702-4133-a0c0-f85a2e90cd74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  1| AAA|dept1|  1000|\n",
            "+---+----+-----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Register DataFrame as Temporary Table\n",
        "df.createOrReplaceTempView(\"temp_table\")\n",
        "\n",
        "# Execute SQL-Like query.\n",
        "spark.sql(\"select * from temp_table where id = 1\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFc3JSVaAnsY",
        "outputId": "605719cf-b01b-4114-af6c-661aa1cf7ba2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  7|\n",
            "|  6|\n",
            "|  9|\n",
            "|  5|\n",
            "|  1|\n",
            "| 10|\n",
            "|  3|\n",
            "|  8|\n",
            "|  2|\n",
            "|  4|\n",
            "+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"select distinct id from temp_table\").show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JwexTz5AnsY",
        "outputId": "ff6c3f88-bf46-4739-db8f-c27ba115a530"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  3| CCC|dept1|  3000|\n",
            "|  4| DDD|dept1|  1500|\n",
            "|  5| EEE|dept2|  8000|\n",
            "|  6| FFF|dept2|  7200|\n",
            "|  7| GGG|dept3|  7100|\n",
            "|  8| HHH|dept3|  3700|\n",
            "|  9| III|dept3|  4500|\n",
            "| 10| JJJ|dept5|  3400|\n",
            "+---+----+-----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"select * from temp_table where salary >= 1500\").show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arjaeZ2-AnsZ"
      },
      "source": [
        "### Leyendo la tabla HIVE como DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIhzo5jIAnsZ"
      },
      "outputs": [],
      "source": [
        "# DB_NAME : Name of the the HIVE Database\n",
        "# TBL_NAME : Name of the HIVE Table\n",
        "\n",
        "\n",
        "df = spark.table(\"DB_NAME\".\"TBL_NAME\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRF1RJFPAnsZ"
      },
      "source": [
        "### Guardar DataFrame como tabla HIVE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQ2AqYV2AnsZ"
      },
      "outputs": [],
      "source": [
        "df.write.saveAsTable(\"DB_NAME.TBL_NAME\")\n",
        "\n",
        "## También podemos seleccionar el argumento \"modo\" con overwrite\", \"append\", \"error\" etc.\n",
        "df.write.saveAsTable(\"DB_NAME.TBL_NAME\", mode=\"overwrite\")\n",
        "\n",
        "# De forma predeterminada, la operación guardará el DataFrame como una tabla interna / administrada de HIVE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYHY0ob6AnsZ"
      },
      "source": [
        "### Guardar el DataFrame como una tabla externa HIVE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fds9-Y1aAnsa"
      },
      "outputs": [],
      "source": [
        "df.write.saveAsTable(\"DB_NAME.TBL_NAME\", path=<location_of_external_table>)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOVebTDZAnsa"
      },
      "source": [
        "### Crea un DataFrame a partir de un archivo CSV\n",
        "* Podemos crear un DataFrame usando un archivo CSV y podemos especificar varias opciones como un separador, encabezado, esquema, inferSchema y varias otras opciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvjS9P0fAnsa"
      },
      "outputs": [],
      "source": [
        " df = spark.read.csv(\"path_to_csv_file\", sep=\"|\", header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdQXP7R0Ansa"
      },
      "source": [
        "### Guardar un DataFrame como un archivo CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fI59MWcmAnsa"
      },
      "outputs": [],
      "source": [
        "df.write.csv(\"path_to_CSV_File\", sep=\"|\", header=True, mode=\"overwrite\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VAnRig4Ansb"
      },
      "source": [
        "### Crea un DataFrame a partir de una tabla relacional\n",
        "* Podemos leer los datos de bases de datos relacionales usando una URL JDBC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzo4wOGgAnsb"
      },
      "outputs": [],
      "source": [
        "# url : a JDBC URL of the form jdbc:subprotocol:subname\n",
        "# TBL_NAME : Name of the relational table.\n",
        "# USER_NAME : user name to connect to DataBase.\n",
        "# PASSWORD: password to connect to DataBase.\n",
        "\n",
        "\n",
        "relational_df = spark.read.format('jdbc')\n",
        "                        .options(url=url, dbtable= <TBL_NAME>, user= <USER_NAME>, password = <PASSWORD>)\n",
        "                        .load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-rb58RwAnsb"
      },
      "source": [
        "### Guardar el DataFrame como una tabla relacional\n",
        "* Podemos guardar el DataFrame como una tabla relacional usando una URL JDBC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0rGusJiAnsb"
      },
      "outputs": [],
      "source": [
        "# url : a JDBC URL of the form jdbc:subprotocol:subname\n",
        "# TBL_NAME : Name of the relational table.\n",
        "# USER_NAME : user name to connect to DataBase.\n",
        "# PASSWORD: password to connect to DataBase.\n",
        "\n",
        "\n",
        " relational_df.write.format('jdbc')\n",
        "                    .options(url=url, dbtable= <TBL_NAME>, user= <USER_NAME>, password = <PASSWORD>)\n",
        "                    .mode('overwrite')\n",
        "                    .save()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}